# Configuration ETL
# Ce fichier contient la configuration pour le pipeline ETL

[ETL]
# Taille des lots pour le traitement
BATCH_SIZE = 100

# Niveau de logging
LOG_LEVEL = INFO

# Chemin des données
DATA_PATH = /app/data

# Mode d'exécution (api, cli, schedule)
MODE = api

# Timeout pour les opérations (en secondes)
OPERATION_TIMEOUT = 300

# Nombre de tentatives en cas d'erreur
MAX_RETRIES = 3

[DATABASE]
# Configuration PostgreSQL
HOST = postgres
PORT = 5432
NAME = scrappingbot
USER = scrappingbot_user
PASSWORD = scrappingbot_pass
SSL_MODE = prefer

# Pool de connexions
MAX_CONNECTIONS = 20
MIN_CONNECTIONS = 5

[REDIS]
# Configuration Redis pour le cache
HOST = redis
PORT = 6379
DB = 0
PASSWORD =
TIMEOUT = 30

[VALIDATION]
# Règles de validation des données
REQUIRED_FIELDS = title,price,url
PRICE_MIN = 50000
PRICE_MAX = 10000000
AREA_MIN = 100
AREA_MAX = 10000

[TRANSFORMATION]
# Configuration des transformations
PRICE_PATTERNS = $,k,K,million
AREA_UNITS = sq ft,square feet,m²,m2
COORDINATE_PRECISION = 6

[API]
# Configuration de l'API ETL
HOST = 0.0.0.0
PORT = 8788
DEBUG = false
WORKERS = 1

# CORS
CORS_ORIGINS = http://localhost:3000,http://localhost:5173,http://localhost:8787

# Authentification (optionnel)
API_KEY_ENABLED = false
API_KEY = 

[MONITORING]
# Configuration du monitoring
HEALTH_CHECK_ENABLED = true
METRICS_ENABLED = true
STATS_RETENTION_DAYS = 30

[SCHEDULING]
# Configuration pour l'exécution programmée
AUTO_RUN_ENABLED = false
CRON_SCHEDULE = 0 2 * * *  # Tous les jours à 2h du matin
FULL_PIPELINE_SCHEDULE = 0 6 * * 0  # Tous les dimanches à 6h
