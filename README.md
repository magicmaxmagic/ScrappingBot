# ScrappingBot

Application containeris√©e de scraping immobilier avec visualisation, monitoring temps r√©el et intelligence artificielle. Architecture moderne Docker avec microservices, base de donn√©es spatiale PostGIS, pipeline ETL, et IA conversationnelle.

## Technologies

- **Docker Multi-Container**: Architecture microservices
- **Base de Donn√©es**: PostgreSQL + PostGIS  
- **ETL Pipeline**: Extract-Transform-Load avec validation qualit√©
- **IA**: Ollama + Chatbot intelligent
- **Scraping**: Playwright avec d√©tection anti-bot
- **API**: FastAPI avec endpoints g√©ospatiaux
- **Frontend**: React + Vite + MapLibre
- **Monitoring**: Grafana + m√©triques temps r√©el
- **Cache**: Redis + Nginx reverse proxy

## Installation Rapide

```bash
# Cloner le projet
git clone https://github.com/magicmaxmagic/ScrappingBot.git
cd ScrappingBot

# Construire et lancer
docker-compose build
docker-compose up -d
```

## Services

| Service | URL | Description |
|---------|-----|-------------|
| **Frontend** | <http://localhost:3000> | Interface web |
| **API** | <http://localhost:8787> | API REST + docs |
| **ETL API** | <http://localhost:8788> | Pipeline ETL + monitoring |
| **Chatbot** | <http://localhost:8080> | Assistant IA |
| **Monitoring** | <http://localhost:3001> | Grafana dashboard |

## Utilisation

### Pipeline ETL (Extract-Transform-Load)

```bash
# Pipeline complet (recommand√©)
make etl-full

# Scraping + ETL combin√©
make scrape-and-etl

# Validation du pipeline
make etl-validate

# D√©monstration avec donn√©es d'exemple
make etl-demo

# Status et rapports
make etl-status

# API ETL (service web)
make etl-api
```

### Scraping

```bash
# Scraping automatique via scheduler
docker-compose logs -f scraper

# Scraping manuel
docker-compose exec scraper python database/scraper_adapter.py --where "Montreal" --what "condo"
```

### Base de Donn√©es

```bash
# Connexion PostgreSQL
docker-compose exec postgres psql -U scrappingbot_user -d scrappingbot

# Requ√™te spatiale exemple
SELECT title, price FROM listings WHERE ST_DWithin(location, ST_Point(-73.567, 45.501), 5000);
```

### Configuration

Cr√©er un fichier `.env`:

```bash
DATABASE_URL=postgresql://scrappingbot_user:password@postgres:5432/scrappingbot
REDIS_URL=redis://redis:6379
OLLAMA_HOST=http://ollama:11434
PLAYWRIGHT_TIMEOUT_MS=30000
```

## Commandes Make

```bash
# Services principaux
make up-all              # Tous les services avec ETL
make up                  # Services essentiels seulement
make down               # Arr√™ter tous les services
make restart            # Red√©marrer tous les services

# Pipeline ETL
make etl-docker-full    # Pipeline ETL complet dans Docker
make etl-docker-demo    # D√©mo ETL dans Docker
make etl-docker-test    # Test ETL Docker complet
make etl-build          # Construire image ETL
make etl-logs           # Voir logs ETL

# Base de donn√©es  
make db-init            # Initialiser la base
make db-shell           # Shell PostgreSQL
make db-backup          # Sauvegarder
make db-restore         # Restaurer

# Scraping
make scrape-test        # Test de scraping
make scrape-montreal    # Scraper Montreal
make scrape-logs        # Voir logs scraper
make manage

# Monitoring en temps r√©el
make monitor-live

# Health check complet
make health-full

# √âtat du syst√®me
make status

# Logs
make logs
```

## Structure

```
ScrappingBot/
‚îú‚îÄ‚îÄ docker/              # Configuration Docker et Dockerfiles
‚îú‚îÄ‚îÄ database/            # Mod√®les PostgreSQL et migrations  
‚îú‚îÄ‚îÄ scraper/            # Scrapers Playwright et spiders
‚îú‚îÄ‚îÄ frontend/           # Interface React et composants
‚îú‚îÄ‚îÄ workers/            # Workers Cloudflare (legacy)
‚îú‚îÄ‚îÄ scripts/            # Scripts de gestion et monitoring
‚îÇ   ‚îú‚îÄ‚îÄ docker/         # Scripts Docker (health, monitor)
‚îÇ   ‚îî‚îÄ‚îÄ manage.sh       # Console de gestion interactive
‚îú‚îÄ‚îÄ docs/               # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
‚îú‚îÄ‚îÄ config/             # Fichiers de configuration
‚îú‚îÄ‚îÄ logs/               # Logs des services
‚îú‚îÄ‚îÄ data/               # Donn√©es temporaires
‚îú‚îÄ‚îÄ docker-compose.yml  # Orchestration des services
‚îî‚îÄ‚îÄ .env.example        # Template variables d'environnement
```

## Tests

```bash
# Tests automatis√©s
docker-compose exec api python -m pytest tests/

# Health check
curl http://localhost:8787/health
```

## Production

```bash
# Mode production
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Sauvegarde
docker-compose exec postgres pg_dump -U scrappingbot_user scrappingbot > backup.sql
```

## Licence

MIT - Voir LICENSE pour d√©tails.

# API
CORS_ORIGINS=http://localhost:3000,http://localhost:5173
```

### Scaling et Performance

```bash
# Scaler le scraper (plusieurs instances)
docker-compose up -d --scale scraper=3

# Monitoring des performances
docker-compose exec grafana grafana-cli admin reset-admin-password admin123
# Puis: http://localhost:3001 (admin/admin123)
```

## Tests et Qualit√©

### Suite de Tests Automatis√©s

```bash
# Tests complets de tous les services
make test-docker

# Tests unitaires d'un service
docker-compose exec api python -m pytest tests/

# Tests d'int√©gration bout-en-bout
docker-compose exec scraper python -m pytest tests/integration/
```

### Monitoring et Observabilit√©

```bash
# M√©triques temps r√©el
docker-compose exec api curl http://localhost:8787/api/stats

# Logs centralis√©s avec timestamps
docker-compose logs -f --timestamps

# Alertes personnalis√©es via Grafana
# http://localhost:3001/alerting/list
```

## D√©ploiement Production

### Optimisations Recommand√©es

```bash
# Build optimis√© pour production
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# Sauvegarde automatique de la base de donn√©es
docker-compose exec postgres pg_dump -U scrappingbot_user scrappingbot > backup.sql

# SSL/HTTPS avec Let's Encrypt (ajout dans nginx)
# Voir documentation nginx/ pour certificats
```

### S√©curit√© et Secrets

```bash
# Utiliser Docker Secrets en production
echo "mot_de_passe_securise" | docker secret create db_password -

# Ou variables d'environnement chiffr√©es
export DATABASE_URL="postgresql://user:$(cat /run/secrets/db_password)@postgres:5432/db"
```

## Cas d'Usage Concrets

### Investisseurs Immobiliers

```bash
# Analyser le march√© Montr√©alais
curl "http://localhost:8787/api/listings?area=montreal&min_price=400000&max_price=800000&property_type=condo"

# Obtenir les statistiques de zone
curl "http://localhost:8787/api/areas/stats"
```

### D√©veloppeurs d'Applications

```bash
# API RESTful compl√®te avec g√©olocalisation
GET /api/listings?bbox=-73.6,45.4,-73.5,45.6  # Requ√™te spatiale
POST /api/listings  # Cr√©er une nouvelle annonce
GET /api/areas/stats  # M√©triques par quartier
```

### Analystes de Donn√©es

```bash
# Export des donn√©es pour analyse
docker-compose exec postgres pg_dump -U scrappingbot_user --table=listings scrappingbot > export.sql

# Connexion directe pour requ√™tes SQL
docker-compose exec postgres psql -U scrappingbot_user -d scrappingbot
```

## ü§ñ Intelligence Artificielle

### Chatbot Conversationnel

Le syst√®me inclut un assistant IA capable de :

- **Recherche intelligente** : "Trouve des condos abordables pr√®s du m√©tro"
- **Analyse de march√©** : "Quel est le prix moyen √† Westmount?"
- **Recommandations** : "Sugg√®re-moi des quartiers familiaux"

```bash
# Exemples d'interaction
curl -X POST http://localhost:8080/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Quels sont les meilleurs quartiers pour investir √† Montr√©al?"}'
```

### Mod√®les LLM Support√©s

- **Llama 3.1** (recommand√©) - 8B param√®tres
- **Mistral 7B** - Alternative rapide
- **CodeLlama** - Pour analyses techniques

## Workflows Automatis√©s

### Scraping Programm√©

```bash
# Le scheduler ex√©cute automatiquement:
# - Scraping quotidien √† 6h00
# - Nettoyage hebdomadaire
# - Backups automatiques
# - Alertes de monitoring

# Personnaliser les horaires:
docker-compose exec scheduler crontab -e
```

### Pipeline ETL Int√©gr√©

1. **Extract** : Scraping via Playwright
2. **Transform** : Normalisation g√©ospatiale  
3. **Load** : Insertion PostgreSQL+PostGIS

## üìä Monitoring et Alertes

### Dashboard Grafana

Acc√©dez √† http://localhost:3001 pour :

- **M√©triques syst√®me** : CPU, RAM, stockage
- **Performance scraping** : Temps r√©ponse, taux succ√®s
- **Base de donn√©es** : Requ√™tes/sec, connexions actives
- **Alertes configurables** : Email, Slack, webhook

### M√©triques Cl√©s

```bash
# Sant√© globale du syst√®me
curl http://localhost:8787/health | jq .

# Statistiques d√©taill√©es
curl http://localhost:8787/api/stats | jq .

# Activit√© r√©cente
curl http://localhost:8787/api/activity?days=7 | jq .
```

## S√©curit√© et √âthique

### Scraping Responsable

- **Respect robots.txt** - V√©rification automatique
- **Rate limiting** - D√©lais entre requ√™tes
- **User-Agent transparent** - Identification claire
- **D√©tection WAF** - Arr√™t automatique si bloqu√©
- **Pas de contournement** - Respect des protections

### S√©curit√© des Donn√©es

- **Chiffrement** des mots de passe base de donn√©es
- **CORS configur√©** pour √©viter XSS
- **Logs s√©curis√©s** - Pas de donn√©es sensibles
- **Backups chiffr√©s** - Sauvegarde automatique

## R√©solution de Probl√®mes

### Probl√®mes Courants

```bash
# Service ne d√©marre pas
docker-compose logs [service-name]

# Base de donn√©es inaccessible
docker-compose exec postgres pg_isready

# Espace disque insuffisant
docker system prune -a
docker volume prune

# Conteneur consomme trop de RAM
docker stats
docker-compose restart [service-name]
```

### WAF D√©tect√© (Normal)

```bash
# Le scraper s'arr√™te automatiquement si bloqu√©
docker-compose logs scraper | grep "WAF detected"

# Attendre et relancer plus tard
sleep 3600 && docker-compose restart scraper
```

### Performance Lente

```bash
# Optimiser PostgreSQL
docker-compose exec postgres psql -U scrappingbot_user -d scrappingbot -c "REINDEX DATABASE scrappingbot;"

# Vider le cache Redis
docker-compose exec redis redis-cli FLUSHALL

# Red√©marrer services
docker-compose restart
```

## Scalabilit√©

### Horizontal Scaling

```bash
# Multiplier les scrapers
docker-compose up -d --scale scraper=5

# Load balancer API
docker-compose up -d --scale api=3
```

### Optimization Base de Donn√©es

```sql
-- Index g√©ospatiaux pour performance
CREATE INDEX idx_listings_location ON listings USING GIST (location);

-- Index sur les prix pour recherches
CREATE INDEX idx_listings_price ON listings (price) WHERE price IS NOT NULL;
```

## D√©ploiement Multi-Environnement

### Environnement de D√©veloppement

```bash
# Mode d√©veloppement avec hot-reload
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d
```

### Production

```bash
# Configuration optimis√©e production
docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

# SSL/TLS avec certificats
# Configurer nginx/ssl.conf avec Let's Encrypt
```

### Cloud (AWS/GCP/Azure)

```bash
# Docker Swarm
docker swarm init
docker stack deploy -c docker-compose.yml scrappingbot

# Kubernetes
kubectl apply -f k8s/
```

---

## Section Legacy (M√©thodes Traditionnelles)

*Cette section conserve les m√©thodes d'installation traditionnelles pour compatibilit√©.*

### Installation Manuelle Python

```bash
# 1. Environnement Python
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 2. Installation Playwright
playwright install

# 3. Base de donn√©es locale
# Installer PostgreSQL + PostGIS localement
```

### Scraping Manuel

```bash
# Scraping simple
python3 scraper/spiders/clean_scraper.py --where "montreal" --what "condo"

# Mode debug avec captures
python3 scraper/spiders/clean_scraper.py --where "test" --what "demo" --dom --debug

# URL personnalis√©e
python3 scraper/spiders/clean_scraper.py \
  --start_url "file://$(pwd)/data/demo_listings.html" \
  --dom --timeout 10000
```

### Frontend D√©veloppement

```bash
# Frontend seul
cd frontend
npm install
npm run dev

# API Workers (Cloudflare)
cd workers  
npm install
npx wrangler dev
```

### Tests Manuels

```bash
# Suite de tests Python
python3 scripts/test_suite.py

# Tests sp√©cifiques
python3 scripts/test_suite.py --test waf_detection -v

# Dashboard statique
python3 scripts/generate_dashboard.py
open logs/dashboard.html
```

---

## Contribution

1. Fork le projet sur GitHub
2. Cr√©er une branche feature : `git checkout -b feature/nouvelle-fonctionnalite`
3. Commiter vos changements : `git commit -am 'Ajout nouvelle fonctionnalit√©'`
4. Pousser vers la branche : `git push origin feature/nouvelle-fonctionnalite`
5. Cr√©er une Pull Request

### Guide de D√©veloppement

```bash
# Setup environnement de d√©veloppement
git clone https://github.com/magicmaxmagic/ScrappingBot.git
cd ScrappingBot

# Lancer en mode dev avec hot-reload
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up -d

# Tests avant commit
make test-docker
make lint
```

## Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de d√©tails.

---

## D√©marrage Rapide - R√©capitulatif

```bash
# Installation compl√®te en 3 commandes
git clone https://github.com/magicmaxmagic/ScrappingBot.git
cd ScrappingBot
docker-compose up -d

# V√©rification que tout fonctionne
curl http://localhost:8787/health
open http://localhost:3000

# Premi√®re utilisation
echo "Votre syst√®me de scraping immobilier est pr√™t !"
echo "Dashboard: http://localhost:3000"
echo "Chatbot IA: http://localhost:8080"
echo "Monitoring: http://localhost:3001"
```

**Pr√™t √† r√©volutionner votre analyse immobili√®re ? **

---

*ScrappingBot - L'avenir de l'analyse immobili√®re automatis√©e* üè†‚ú®
